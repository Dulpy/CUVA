{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "\n",
    "sys.path.append('../ECVA')\n",
    "from AnomShield.conversation import conv_templates\n",
    "from AnomShield.constants import DEFAULT_MMODAL_TOKEN, MMODAL_TOKEN_INDEX\n",
    "from AnomShield.mm_utils import get_model_name_from_path, tokenizer_MMODAL_token, process_video, process_image\n",
    "from AnomShield.model.builder import load_pretrained_model\n",
    "\n",
    "model_path = 'The path to the final model'\n",
    "\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "\n",
    "model_base = 'The path to the Mistral-7B-Instruct-v0___2'\n",
    "\n",
    "tokenizer, model, processor, context_len = load_pretrained_model(model_path, model_base, model_name)\n",
    "model = model.to('cuda')\n",
    "\n",
    "conv_mode = 'mistral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "lora_weight = 'The path to the lora weight'\n",
    "print(f\"Loading LoRA weights from {lora_weight}\")\n",
    "peft_model = PeftModel.from_pretrained(model, lora_weight).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pretrain_mm_mlp_adapter_v2 = 'The path to the projecter weight'\n",
    "\n",
    "mm_projector_weights = torch.load(pretrain_mm_mlp_adapter_v2, map_location='cpu')\n",
    "\n",
    "\n",
    "def get_w(weights, keyword):\n",
    "    return {k.split(keyword + '.')[1]: v for k, v in weights.items() if keyword in k}\n",
    "\n",
    "peft_model.base_model.model.model.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(question, video):\n",
    "    # Video Inference\n",
    "    paths = [video]\n",
    "    questions = [question]\n",
    "    \n",
    "    if '.jpg' in video or 'png' in video:\n",
    "        modal_list = ['image']\n",
    "    else:\n",
    "        modal_list = ['video']\n",
    "    # Visual preprocess (load & transform image or video).\n",
    "    if modal_list[0] == 'video':\n",
    "        tensor = process_video(paths[0], processor, model.config.image_aspect_ratio, num_frames=16).to(dtype=torch.float16, device='cuda', non_blocking=True)\n",
    "        default_mm_token = DEFAULT_MMODAL_TOKEN[\"VIDEO\"]\n",
    "        modal_token_index = MMODAL_TOKEN_INDEX[\"VIDEO\"]\n",
    "    else:\n",
    "        tensor = process_image(paths[0], processor, model.config.image_aspect_ratio)[0].to(dtype=torch.float16, device='cuda', non_blocking=True)\n",
    "        default_mm_token = DEFAULT_MMODAL_TOKEN[\"IMAGE\"]\n",
    "        modal_token_index = MMODAL_TOKEN_INDEX[\"IMAGE\"]\n",
    "    tensor = [tensor]\n",
    "\n",
    "    # Text preprocess (tag process & generate prompt).\n",
    "    question = default_mm_token + \"\\n\" + questions[0]\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], question)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    input_ids = tokenizer_MMODAL_token(prompt, tokenizer, modal_token_index, return_tensors='pt').unsqueeze(0).to('cuda')\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = peft_model.generate(\n",
    "            input_ids,\n",
    "            images_or_videos=tensor,\n",
    "            modal_list=modal_list,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            max_new_tokens=1024,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'The path to the video'\n",
    "\n",
    "question = \"Introduce this video\"\n",
    "res = inference(question,video_path)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VideoLLaMA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
