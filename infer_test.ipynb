{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "sys.path.append('../MLLM')\n",
    "from conversation import conv_templates\n",
    "from constants import DEFAULT_MMODAL_TOKEN, MMODAL_TOKEN_INDEX\n",
    "from mm_utils import get_model_name_from_path, tokenizer_MMODAL_token, process_video, process_image\n",
    "from model.builder import load_pretrained_model\n",
    "\n",
    "model_path = '/data/vllm/VideoLLaMA2-main/work_dirs/mamba_pretrain_v1/mamba_pretrain_v1'\n",
    "\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "\n",
    "model_base = '/data/vllm/VideoLLaMA2-main/mistral_ckpt/Mistral-7B-Instruct-v0___2'\n",
    "\n",
    "tokenizer, model, processor, context_len = load_pretrained_model(model_path, model_base, model_name)\n",
    "model = model.to('cuda')\n",
    "\n",
    "conv_mode = 'mistral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA weights from /data/vllm/VideoLLaMA2-main/work_dirs/mamba_sft/finetune_mamba_sft_91_2stage_1mamba_2mlp_add_pos\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "lora_weight = '../work_dirs/mamba_sft/finetune_mamba_sft_91_2stage_1mamba_2mlp_add_pos'\n",
    "print(f\"Loading LoRA weights from {lora_weight}\")\n",
    "peft_model = PeftModel.from_pretrained(model, lora_weight).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pretrain_mm_mlp_adapter_v2 = '../work_dirs/mamba_sft/finetune_mamba_sft_91_2stage_1mamba_2mlp_add_pos/non_lora_trainables.bin'\n",
    "\n",
    "mm_projector_weights = torch.load(pretrain_mm_mlp_adapter_v2, map_location='cpu')\n",
    "\n",
    "\n",
    "def get_w(weights, keyword):\n",
    "    return {k.split(keyword + '.')[1]: v for k, v in weights.items() if keyword in k}\n",
    "\n",
    "peft_model.base_model.model.model.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(question, video):\n",
    "    # Video Inference\n",
    "    paths = [video]\n",
    "    questions = [question]\n",
    "    \n",
    "    if '.jpg' in video or 'png' in video:\n",
    "        modal_list = ['image']\n",
    "    else:\n",
    "        modal_list = ['video']\n",
    "    # Visual preprocess (load & transform image or video).\n",
    "    if modal_list[0] == 'video':\n",
    "        tensor = process_video(paths[0], processor, model.config.image_aspect_ratio, num_frames=16).to(dtype=torch.float16, device='cuda', non_blocking=True)\n",
    "        default_mm_token = DEFAULT_MMODAL_TOKEN[\"VIDEO\"]\n",
    "        modal_token_index = MMODAL_TOKEN_INDEX[\"VIDEO\"]\n",
    "    else:\n",
    "        tensor = process_image(paths[0], processor, model.config.image_aspect_ratio)[0].to(dtype=torch.float16, device='cuda', non_blocking=True)\n",
    "        default_mm_token = DEFAULT_MMODAL_TOKEN[\"IMAGE\"]\n",
    "        modal_token_index = MMODAL_TOKEN_INDEX[\"IMAGE\"]\n",
    "    tensor = [tensor]\n",
    "\n",
    "    # Text preprocess (tag process & generate prompt).\n",
    "    question = default_mm_token + \"\\n\" + questions[0]\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], question)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    input_ids = tokenizer_MMODAL_token(prompt, tokenizer, modal_token_index, return_tensors='pt').unsqueeze(0).to('cuda')\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = peft_model.generate(\n",
    "            input_ids,\n",
    "            images_or_videos=tensor,\n",
    "            modal_list=modal_list,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            max_new_tokens=1024,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '/data/vllm/datasets/othervideos/combine_dataset/00073.mp4'\n",
    "\n",
    "question = \"Please analyze the content of the video and comprehensively summarize the reasons causing the abnormal events. Ensure that you cover all possible reasons for the abnormal events as comprehensively as possible. The output format must strictly follow the following formats:1.xxxx\\n2.xxxxx\\nFor example,1.The quarrel between the two men became more and more serious.\\n2.The car that collided was speeding.\\n3.The thief wanted to steal valuables.\"\n",
    "res = inference(question,video_path)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VideoLLaMA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
